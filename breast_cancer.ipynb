{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2) We will perform an experiment in this problem. Experiments help you make data-driven decisions to maximize the performance of your ML pipeline. The goal of this experiment is two-fold. We will study which ML algorithms require preprocessing and which algorithm performs best on the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) The breast cancer dataset is loaded directly from sklearn. Please read the description of the dataset [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). \n",
    "\n",
    "Split the data into train and test sets with 25% of points in test such that the split is reproducable (the same points end up in train and test every time you run your code). All features are continuous. Leave the continuous features unprocessed for now! (2 points)\n",
    "\n",
    "Train a Lasso, a random forest, an SVM rbf, and a nearest neighbor model on the data. Please tune the appropriate parameters and evaluate the performance on the test set. Use accuracy as your evaluation metric. Print which parameter values give the best prediction. Determine the parameter range such that the best value is not near the lowest or largest values if possible. The parameters should span a wide range of values. Print the best test score for each model! (12 points) \n",
    "\n",
    "Which ML algorithm performs best on the data? (1 point)\n",
    "\n",
    "Hint: to avoid code duplication and potential bugs (see 2b), please wrap the four methods and the parameter tuning into a function which takes X_train, y_train, X_test, y_test as an input, and it returns the best Lasso, random forest, SVM, and nearest neighbor test scores.\n",
    "\n",
    "Hint 2: Randomforest is non-deterministic. Make sure that your results are reproducable such that if you run your code again, the same scores and best parameters are returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569\n",
      "(569, 30)\n",
      "30\n",
      "['malignant' 'benign']\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(len(y))\n",
    "print(np.shape(X))\n",
    "ftr_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "print(len(ftr_names))\n",
    "print(target_names)\n",
    "print(ftr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: The best test accuracy score is 0.965034965034965 and the alpha values are 0.001 the C value is 1000.0\n",
      "Random forest: The best test score is 0.958041958041958 and the corresponding depth and min_samples_split values are [[5 5]\n",
      " [5 6]\n",
      " [5 7]]\n",
      "SVC rbf: The best test score of is 0.986013986013986 and the corresponding gamma and C values are [[1.e-09 1.e+05]\n",
      " [1.e-08 1.e+04]\n",
      " [1.e-07 1.e+03]\n",
      " [1.e-06 1.e+02]]\n",
      "KNN: The best accuracy score is 0.9790209790209791 , and the num of neighbors is 11\n"
     ]
    }
   ],
   "source": [
    "def train(X_train, X_test, y_train, y_test):\n",
    "    # lasso\n",
    "    alpha = np.logspace(-3,5,20)\n",
    "    a = []\n",
    "    for i in range(len(alpha)):\n",
    "        log_reg_l1 = LogisticRegression(penalty='l1',C = 1/alpha[i], solver='saga', max_iter = 1e4)\n",
    "        log_reg_l1.fit(X_train, y_train)\n",
    "        y_pred = log_reg_l1.predict(X_test)\n",
    "        a.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"Lasso: The best test accuracy score is\", max(a), \n",
    "          \"and the alpha values are\", alpha[a.index(max(a))], \n",
    "          \"the C value is\", 1/alpha[a.index(max(a))])\n",
    "    \n",
    "    # random forest\n",
    "    depths = [i for i in range(1,11)]\n",
    "    sss = [i for i in range(2,13)]\n",
    "    a = []\n",
    "    pair = []\n",
    "    for depth in depths:\n",
    "        for ss in sss:\n",
    "            rfr = RandomForestClassifier(n_estimators=1, max_depth=depth, min_samples_split=ss, random_state=1)\n",
    "            rfr.fit(X_train, y_train)\n",
    "            y_pred = rfr.predict(X_test)\n",
    "            pair.append((depth,ss))\n",
    "            a.append(accuracy_score(y_test, y_pred))\n",
    "    pair_arr = np.array(pair)   \n",
    "    print(\"Random forest: The best test score is\",max(a), \n",
    "      \"and the corresponding depth and min_samples_split values are\", pair_arr[a == max(a)])\n",
    "    \n",
    "    # SVC rbf\n",
    "    gammas= np.logspace(-9, 3, 13)\n",
    "    Cs = np.logspace(-2, 10, 13)\n",
    "    a = []\n",
    "    pair = []\n",
    "    for gamma_value in gammas:\n",
    "        for c in Cs:\n",
    "            svc = SVC(gamma = gamma_value, C = c, probability=True)\n",
    "            svc.fit(X_train, y_train)\n",
    "            pair.append((gamma_value,c))\n",
    "            y_pred = svc.predict(X_test)\n",
    "            a.append(accuracy_score(y_test, y_pred))\n",
    "    pair_arr = np.array(pair)   \n",
    "    print(\"SVC rbf: The best test score of is\",max(a), \n",
    "      \"and the corresponding gamma and C values are\", pair_arr[a == max(a)])\n",
    "    \n",
    "    # KNN\n",
    "    a = []\n",
    "    n_n = [i for i in range(1,15)]\n",
    "#     n_n = range(1,15)\n",
    "    for n in n_n:\n",
    "        knc = KNeighborsClassifier(n_neighbors=n)\n",
    "        knc.fit(X_train, y_train)\n",
    "        y_pred = knc.predict(X_test)\n",
    "        a.append(accuracy_score(y_test, y_pred))\n",
    "#     plt.plot(n_n, a)\n",
    "#     plt.show()\n",
    "    print(\"KNN: The best accuracy score is\", max(a), \n",
    "          \", and the num of neighbors is\", n_n[a.index(max(a))])\n",
    "    \n",
    "train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which ML algorithm performs best on the data? (1 point)    \n",
    "\n",
    "Ans: The SVC rbf performs best on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) Now that our modeling approach has been developed, we change one aspect of it and we will study how this one change impacts the accuracy scores. This is the key to a successful experiment. \n",
    "\n",
    "The one aspect we change: standardize the continuous features! Take care to avoid data leakage! (3 points) \n",
    "\n",
    "Repeat the proceduce in 2a on the standardized dataset. (0 points)\n",
    "\n",
    "Which model gives the best accuracy score this time? How much improvement in accuracy did we gain by preprocessing? (2 point)\n",
    "\n",
    "The accuracy score of which algorithm improved the most after preprocessing? (1 point)\n",
    "\n",
    "Which algorithm is insensitive to preprocessing? (1 point)\n",
    "\n",
    "We will discuss these findings in class after the submission deadline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: The best test accuracy score is 0.972027972027972 and the alpha values are 0.001 the C value is 1000.0\n",
      "Random forest: The best test score is 0.951048951048951 and the corresponding depth and min_samples_split values are [[5 5]\n",
      " [5 6]\n",
      " [5 7]]\n",
      "SVC rbf: The best test score of is 0.993006993006993 and the corresponding gamma and C values are [[1.e-08 1.e+10]]\n",
      "KNN: The best accuracy score is 0.972027972027972 , and the num of neighbors is 6\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "train(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model gives the best accuracy score this time? How much improvement in accuracy did we gain by preprocessing? (2 point)   \n",
    "\n",
    "Ans: The SVC rbf performs the best after preprocessing. Lasso increases by 0.7%, Random forest stays the same, SVC rbf increases by 0.71%, KNN decreases by 0.7%. \n",
    "\n",
    "The accuracy score of which algorithm improved the most after preprocessing? (1 point)    \n",
    "\n",
    "Ans: SVC rbf improves the most.\n",
    "\n",
    "Which algorithm is insensitive to preprocessing? (1 point)    \n",
    "\n",
    "Ans: Random forest is insensitive to preprocessing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
